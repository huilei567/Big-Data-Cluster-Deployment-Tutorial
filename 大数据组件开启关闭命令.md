# 开启关闭命令

## Zookeeper

- 开启

master

```shell
zkServer.sh start
```

slave1

```shell
zkServer.sh start
```

slave2

```shell
zkServer.sh start
```

- 关闭

master

```shell
zkServer.sh stop
```

slave1

```shell
zkServer.sh stop
```

slave2

```shell
zkServer.sh stop
```

## Hadoop

- 开启

Hadoop HA先需要开启zookeeper

```shell
cd $HADOOP_HOME
sbin/start-dfs.sh
sbin/start-yarn.sh
```

- 关闭

```shell
cd $HADOOP_HOME
sbin/stop-dfs.sh
sbin/stop-yarn.sh
```

## Spark

spark on yarn 直接运行即可，不需要开启

## Hbase

- 开启

需要先开启hadoop和zookeeper

```shell
start-hbase.sh
```

- 进入shell

```shell
hbase shell
```

- 关闭

```shell
stop-hbase.sh
```

## Hive

需要先开启hadoop

使用Metastore 服务，进入控制台

```shell

hive
```

启动 Metastore 服务

```shell

hive --service metastore
```

[更多](https://blog.csdn.net/lblblblblzdx/article/details/79760959)

## Redis

- 开启

```shell
# 带配置文件开启
cd /opt/module/redis-6.2.6
redis-server redis.conf

# 进入控制台
redis-cli
```

- 关闭

```shell
redis-cli shutdown
```

## Azkaban

- 开启

master

```shell
# 开启执行器
cd $AZKABAN_EXEC_HOME
bin/start-exec.sh
# 状态设置为激活
curl -G "localhost:$(<$AZKABAN_EXEC_HOME/executor.port)/executor?action=activate" && echo
```

slave1

```shell
# 开启执行器
cd $AZKABAN_EXEC_HOME
bin/start-exec.sh
# 状态设置为激活
curl -G "localhost:$(<$AZKABAN_EXEC_HOME/executor.port)/executor?action=activate" && echo
```

slave2

```shell
# 开启执行器
cd $AZKABAN_EXEC_HOME
bin/start-exec.sh
# 状态设置为激活
curl -G "localhost:$(<$AZKABAN_EXEC_HOME/executor.port)/executor?action=activate" && echo
```

master

```shell
# 启动网页服务器
cd $AZKABAN_WEB_HOME
bin/start-web.sh
```

- 关闭

```shell
# 关闭网页服务器
cd $AZKABAN_WEB_HOME
bin/shutdown-web.sh
```

master

```shell
# 关闭执行器
cd $AZKABAN_EXEC_HOME
bin/shutdown-exec.sh
```

slave1

```shell
# 关闭执行器
cd $AZKABAN_EXEC_HOME
bin/shutdown-exec.sh
```

slave2

```shell
# 关闭执行器
cd $AZKABAN_EXEC_HOME
bin/shutdown-exec.sh
```

## flume

- 使用

```shell
flume-ng agent --conf conf --conf-file conf文件地址 --name agent -Dflume.root.logger=INFO,console
```

## Kafka

先开启zookeeper

- 开启

master

```shell
kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties
```

slave1

```shell
kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties
```

slave2

```shell
kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties
```

- 关闭

master

```shell
kafka-server-stop.sh
```

slave1

```shell
kafka-server-stop.sh
```

slave2

```shell
kafka-server-stop.sh
```
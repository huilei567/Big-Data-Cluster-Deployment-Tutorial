# 安装配置docker ce  
## 安装docker  
### 更新本地软件源索引  
```shell
sudo apt-get update
```  
### 安装必要软件  
```shell
sudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common`  
```
### 安装阿里云docker-ce gpg证书  
```shell
curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -
```
### 写入阿里源 docker ce镜像信息  
```shell
sudo add-apt-repository "deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable"  
```
### 更新本地软件源索引  
```shell
sudo apt-get update
```
### 安装docker-ce  
```shell
sudo apt-get -y install docker-ce
```  
检查(查看docker ce 版本)  
```shell
docker version
``` 
## 启动docker-ce  
```shell
sudo service docker start
```  
### 创建docker用户组(可省略)  
```shell
sudo groupadd docker
```  
### 添加当前用户到Docker用户组  
```shell
sudo gpasswd -a $USER docker
``` 
### 更新用户组  
```shell
newgrp docker
```  
### 创建网络  
```shell
docker network create --subnet=172.18.0.0/16   mynetwork
```
检查  
```shell
docker network ls
```
## 创建centos7虚拟机 
```shell
docker run -itd --name staticIp-privileged --net mynetwork --ip 172.18.0.2 --privileged=true centos:7 /sbin/init
```  
检查  
```shell
docker ps -a
```  
### 登录虚拟机  
```shell
docker exec -it staticIp-privileged /bin/bash
```  
### 安装必要软件  
```shell
yum install net-tools openssh-server openssh-clients initscripts vim -y
```   
## 开启ssh服务  
### 编辑ssh客户端配置文件  
```shell
vim /etc/ssh/sshd_config
```   
`i` 进入编辑模式  
删除以下行的#  
```
#Port 22
#ListenAddress 0.0.0.0
#ListenAddress ::
#PermitRootLogin yes
```
`esc` 退出编辑模式  
`:wq`  保存退出  
### 编辑ssh服务器配置文件  
```shell
vim /etc/ssh/ssh_config
```  
`i` 进入编辑模式  
删除以下行的#
```
#PasswordAuthenticatioon yes
```  
`esc` 退出编辑模式  
`:wq` 保存退出  

### 启动ssh服务  
```shell
systemctl start sshd.service
```  
### 查看ssh服务状态(`active (running)`为成功运行)  
```shell
systemctl status sshd.service
```  
`q` 退出(可能需要退出状态列表)  
设置root用户密码  
```shell
passwd
```  
### 退出当前虚拟机    
```shell
exit
```  
## 实现Ubuntu虚拟机和Windows,  Ubuntu虚拟机与docker虚拟机之间的ssh互通
尝试用Ubuntu登录docker容器  
```shell
ssh root@172.18.0.2
```  
退出ssh登录
```shell
exit
```   
尝试用Windows登录Ubuntu虚拟机  
```
ssh root@Localhost
```
### Ubuntu上安装ssh服务器
同上  
### 将Ubuntu的/opt的权限改为777，便于Windows将文件传入此处  
```shell
sudo chmod 777 /opt
```  
### 将ubantu宿主机中的安装包传输镜像文件中  
```shell
docker cp /opt/package/ staticIp-privileged:/opt
``` 
### 将docker容器 保存为  docker镜像 centos:hadoop
```shell
docker stop staticIp-privileged
docker images
docker commit -m "first docker" staticIp-privileged centos:hadoop
docker images
```
### 使用docker镜像centos:hadoop,创建虚拟机master,slave1,salve2。
```shell
docker ps -a
docker run -itd --name master --net mynetwork --ip 172.18.0.10 --privileged=true centos:hadoop /sbin/init
docker run -itd --name slave1 --net mynetwork --ip 172.18.0.11 --privileged=true centos:hadoop /sbin/init
#slave2将在数仓部署中充当MySQL服务器,为避免cgroup内核限制资源目录,需要加上"-v /sys/fs/cgroup:/sys/fs/cgroup"
docker run -itd --name slave2 --net mynetwork --ip 172.18.0.12 --privileged=true -v /sys/fs/cgroup:/sys/fs/cgroup centos:hadoop /sbin/init 
docker ps -a
```
## 节点之间的开启ssh免密登录的先前准备  
### 进入master节点  
操作环境: master、slave1、slave2  
```shell
docker exec -it master /bin/bash
```  
### 测试hosts设置
```shell
ping master
^C
ping slave1
^C
ping slave2
^C
```
### 在centos开机时加入自启动脚本（docker容器修改hosts文件，重启失效）  
```shell
vim /etc/profile.d/host.sh
```  
`i` 进入编辑模式 
```shell
echo -e "172.18.0.10 master\n172.18.0.11 slave1\n172.18.0.12 slave2" > /etc/hosts
```  
`esc` 退出编辑模式  
`:wq`  保存退出

### 时区同步  
```shell
tzselect
```  
`5911`  
获取对应环境变量“`TZ='Asia/Shanghai'; export TZ`”   
### 设置TZ环境变量  
```shell
vim /etc/profile.d/set_time.sh
```  
`i` 进入编辑模式  
```shell
TZ='Asia/Shanghai'; export TZ
```  
`esc` 退出编辑模式  
`:wq` 保存退出  
应用环境变量  
```shell
source /etc/profile
```  
测试  
```shell
date
``` 
(初始为UTC，改后为CST)

## NTP同步  
 `yum -y install ntp`  
屏蔽默认server，设置master为本地时钟源，服务器层级设为10  
`vim /etc/ntp.conf `  
切到最后一行
`i` 进入编辑模式 
```
server 127.127.1.0
fudge 127.127.1.0 stratum 10
```
`esc` 退出编辑模式  
`:wq` 保存退出  
### 重启ntp服务  
```shell
systemctl restart ntpd.service
```  
### slaves手动同步时间（slave1，slave2上 ）：  
### 退出master容器
```shell
exit
```   
### 进入slave1容器  
```shell
docker exec -it slave1 /bin/bash
```  
### 重复进入master后的部分  
```shell
ntpdate master
```  
### 退出slave1容器
```shell
exit
```   
### 进入slave1容器  
```shell
docker exec -it slave2 /bin/bash
```  
### 重复进入master后的部分  
```shell
ntpdate master
```
### 退出slave2容器
```shell
exit
``` 

## 开启三台容器间的免密登录  
```shell
docker exec -it master /bin/bash
```  
### 生成SSH密钥对  
```shell
ssh-keygen
```
输入密钥保存地址，空为默认    
输入passphrase，可为空  
确认passphrase  
建⽴master⾃身使⽤root⽤户ssh访问localhost免密登录  
```shell
ssh-copy-id localhost
```  
`yes`  
root@master的密码  
### 建⽴master使⽤root⽤户到slave1和slave2的ssh免密登录访问  
```shell
ssh-copy-id slave1
```  
`yes`  
root@slave1的密码  
```shell
ssh-copy-id slave2
```  
`yes`  
root@slave2的密码  

# 任务一：Hadoop 完全分布式安装配置  
## 安装java环境
### 将Master节点JDK安装包解压并移动到/usr/java路径(若路径不存在，则需新建)  
```shell
mkdir -p /usr/java
cd /opt/package/
tar -zxvf jdk-8u171-linux-x64.tar.gz -C /usr/java/
```
### 配置Java环境变量  
```shell
vim /etc/profile.d/java.sh
```  
`i` 进入编辑模式  
```shell
#java
export JAVA_HOME=/usr/java/jdk1.8.0_171
export PATH=$PATH:$JAVA_HOME/bin
```
`esc` 退出编辑模式  
`:wq` 保存退出  
### 应用环境变量  
```shell
source /etc/profile
```  
测试
```shell
java 
javac
```  
### 分发/usr/java文件给slave1，slave2  
```shell
scp -r /usr/java/ root@slave1:/usr/
scp -r /usr/java/ root@slave2:/usr/
scp -r /etc/profile.d root@slave1:/etc
scp -r /etc/profile.d root@slave2:/etc
ssh root@slave1 'source /etc/profile'
ssh root@slave2 'source /etc/profile'
```
测试
```shell
ssh root@slave1 'java'
ssh root@slave1 'javac'
ssh root@slave2 'java'
ssh root@slave2 'javac'
```
## 安装Hadoop
### 将Hadoop安装包解压到指定路径  
```shell
mkdir -p /usr/hadoop
tar -zxvf /opt/package/hadoop-2.7.7.tar.gz -C /usr/hadoop/
```
### 配置Hadoop环境变量  
`vim /etc/profile.d/hadoop.sh`
`i` 进入编辑模式
```shell
#hadoop
export HADOOP_HOME=/usr/hadoop/hadoop-2.7.7
export CLASSPATH=$CLASSPATH:$HADOOP_HOME/lib
export PATH=$PATH:$HADOOP_HOME/bin
export HADOOP_CLASSPATH=`hadoop classpath`
```
`esc` 退出编辑模式  
`:wq` 保存退出
### 应用环境变量  
`source /etc/profile`  
切到/usr/hadoop/hadoop-2.7.7/etc/hadoop文件夹
```shell
cd $HADOOP_HOME/etc/hadoop
ll
```
## 修改Hadoop配置文件  
### 修改`hadoop-env.sh`  
配置Hadoop运行环境JAVA_HOME  
```shell
vim hadoop-env.sh
```  
切到第28行("`# The jsvc implementation to use. Jsvc is required to run secure datanodes`"下面)  
`i` 进入编辑模式
```shell
export JAVA_HOME=/usr/java/jdk1.8.0_171
```
`esc` 退出编辑模式
`:wq` 保存退出

### 修改`core-site.xml`
设置全局参数，指定NN的IP为master（映射名），端口为`9000`  
指定存放临时数据的目录为hadoop安装目录下`/hdfs/tmp`(绝对路径，下同)  
```shell
vim core-site.xml
```
切到“`<configuration></configuration>`”中间  
`i` 进入编辑模式
```xml
<property>
  <name>fs.default.name</name>
   <value>hdfs://master:9000</value>
</property>
<property>
  <name>hadoop.tmp.dir</name>
   <value>/usr/hadoop/hadoop-2.7.7/hdfs/tmp</value>
<description>A base for other temporary directories.</description>
</property>
<property>
  <name>io.file.buffer.size</name>
   <value>131072</value>
</property>
<property>
  <name>fs.checkpoint.period</name>
   <value>60</value>
</property>
<property>
  <name>fs.checkpoint.size</name>
   <value>67108864</value>
</property>
```
`esc` 退出编辑模式
`:wq` 保存退出

### 修改`hdfs-site.xml`  
设置HDFS参数
指定备份文本数量为`2`  
指定NN存放元数据信息路径为hadoop目录下`/hdfs/name`  
指定DN存放元数据信息路径为hadoop安装目录下`/hdfs/data`  
```shell
vim hdfs-site.xml
```
切到“`<configuration></configuration>`”中间  
`i` 进入编辑模式  
```xml
<property>
 <name>dfs.replication</name>
   <value>2</value>
 </property>
 <property>
   <name>dfs.namenode.name.dir</name>
   <value>file:/usr/hadoop/hadoop-2.7.7/hdfs/name</value>
   <final>true</final>
 </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:/usr/hadoop/hadoop-2.7.7/hdfs/data</value>
    <final>true</final>
 </property>
 <property>
  <name>dfs.namenode.secondary.http-address</name>
   <value>master:9001</value>
 </property>
 <property>
   <name>dfs.webhdfs.enabled</name>
    <value>true</value>
 </property>
 <property>
   <name>dfs.permissions</name>
   <value>false</value>
</property>
```
`esc` 退出编辑模式  
`:wq` 保存退出

### 修改`yarn-env.sh`  
设置`YARN`运行环境`JAVA_HOME`参数  
切到第23行(# `some Java parameters`下面)  
```shell
vim yarn-env.sh
```
`i` 进入编辑模式
```shell
export JAVA_HOME=/usr/java/jdk1.8.0_171
```
`esc` 退出编辑模式  
`:wq` 保存退出

### 修改`yarn-site.xml`
设置`YARN`核心参数，
指定`ResourceManager`进程所在主机为`master`，端口为`18141`  
指定`NodeManager`上运行的附属服务为`shuffl` 
```shell
vim yarn-site.xml
```
切到“`<configuration></configuration>`”中间
`i` 进入编辑模式  
```xml
<property>
 <name>yarn.resourcemanager.address</name>
   <value>master:18040</value>
</property>
<property>
  <name>yarn.resourcemanager.scheduler.address</name>  <value>master:18030</value>
</property>
<property>
  <name>yarn.resourcemanager.webapp.address</name>
  <value>master:18088</value>
</property>
<property>
  <name>yarn.resourcemanager.resource-tracker.address</name>
  <value>master:18025</value>
</property>
<property>
 <name>yarn.resourcemanager.admin.address</name>
 <value>master:18141</value>
</property>
<property>
 <name>yarn.resourcemanager.aux-services</name>
 <value>mapreduce_shuffle</value>
</property>
<property>
 <name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name>
 <value>org.apache.hadoop.mapred.shuffleHandler</value>
</property>
<property>
 <name>yarn.nodemanager.aux-services</name>
 <value>mapreduce_shuffle</value>
</property>
```
`esc` 退出编辑模式  
`:wq` 保存退出

### 创建并修改`mapred-site.xml`  
设置计算框架参数，指定`MR`运行在`yarn`上  
```shell
cp mapred-site.xml.template mapred-site.xml
vim mapred-site.xml
```
切到“`<configuration></configuration>`”中间
`i` 进入编辑模式  
```xml
<property>
   <name>mapreduce.framework.name</name>
   <value>yarn</value>
</property>
``` 
`esc` 退出编辑模式  
`:wq` 保存退出  

节点文件设置  
修改`slaves` 
```shell
vim slaves
```
`i` 进入编辑模式  
将内容改为
```
slave1
slave2
```
`esc` 退出编辑模式  
`:wq` 保存退出  
创建并修改`master`  
```shell
vim master
``` 
`i` 进入编辑模式  
```
master
```
`esc` 退出编辑模式  
`:wq` 保存退出

### `namenode`格式化
```shell
scp -r /etc/profile.d/hadoop.sh root@slave1:/etc/profile.d
scp -r /etc/profile.d/hadoop.sh root@slave2:/etc/profile.d
scp -r /usr/hadoop root@slave1:/usr
scp -r /usr/hadoop root@slave2:/usr
ssh root@slave1 'source /etc/profile'
ssh root@slave2 'source /etc/profile'
hadoop namenode -format
```
倒数第五行有“`Exiting with status 0`”，表示成功

## 启动hadoop集群  
```shell
$HADOOP_HOME/sbin/start-all.sh 
```
### 在Ubuntu打开namenode的web界面
```shell
google-chrome 172.18.0.10:50070
```
# 任务二 spark安装

## 安装scala  
### 创建scala工作目录  
```shell
mkdir -p /usr/scala
tar -xvf /opt/package/scala-2.11.12.tgz -C /usr/scala
```
### 配置`Scala`环境变量
```shell
vim /etc/profile.d/scala.sh
```
`i` 进入编辑模式
```shell
#scala
export SCALA_HOME=/usr/scala/scala-2.11.12
export PATH=$SCALA_HOME/bin:$PATH
```  
`esc` 退出编辑模式  
`:wq` 保存退出
### 应用环境变量
```shell
source /etc/profile
```
测试  
```shell
scala -version
```
### 分发到slaves
```shell
scp -r /usr/scala root@slave1:/usr
scp -r /usr/scala root@slave2:/usr
scp -r /etc/profile.d/scala.sh root@slave1:/etc/profile.d
scp -r /etc/profile.d/scala.sh root@slave2:/etc/profile.d
ssh root@slave1 'source /etc/profile'
ssh root@slave1 'source /etc/profile'
```
测试
```shell
ssh root@slave1 'scala -version'
ssh root@slave2 'scala -version'
```


## 安装Spark  
### 创建spark工作目录  
```shell
mkdir -p /usr/spark
tar -zxvf /opt/package/spark-2.1.1-bin-hadoop2.7.tgz -C /usr/spark
```

### 配置spark环境变量  
```shell
vim /etc/profile.d/spark.sh
```
`i` 进入编辑模式
```shell 
#spark
export SPARK_HOME=/usr/spark/spark-2.1.1-bin-hadoop2.7
export PATH=$SPARK_HOME/bin:$PATH
```
`esc` 退出编辑模式
`:wq` 保存退出
### 生效环境变量
```shell
source /etc/profile
```
## 修改Spark配置文件  
```shell
cd /usr/spark/spark-2.1.1-bin-hadoop2.7/conf
```
编辑`spark-env.sh`  
```shell
cp spark-env.sh.template spark-env.sh
vim spark-env.sh
```
切到22行
`i` 进入编辑模式  
```shell
export SPARK_MASTER_IP=master
export SCALA_HOME=/usr/scala/scala-2.11.12
export SPARK_WORKER_MEMORY=8g
export JAVA_HOME=/usr/java/jdk1.8.0_171
export HADOOP_HOME=/usr/hadoop/hadoop-2.7.7
export HADOOP_CONF_DIR=/usr/hadoop/hadoop-2.7.3/etc/hadoop
```
`esc` 退出编辑模式  
`:wq` 保存退出  

### 配置spark从节点
```shell
cp slaves.template slaves
vim slaves
```
`i` 进入编辑模式
切到第19行("`localhost`")  
删除localhost  
```
slave1
slave2
```
`esc` 退出编辑模式  
`:wq` 保存退出  

### 分发到slaves
```shell
scp -r /usr/spark root@slave1:/usr
scp -r /usr/spark root@slave2:/usr
scp -r /etc/profile.d/spark.sh root@slave1:/etc/profile.d
scp -r /etc/profile.d/spark.sh root@slave2:/etc/profile.d
ssh root@slave1 'source /etc/profile'
ssh root@slave2 'source /etc/profile'
```
## 启动spark集群
```shell
$SPARK_HOME/sbin/start-all.sh
```
`jps`查看进程  
`master`中有  
xxxx `Master`  
`slaves`中有  
xxxx `Worker`  

访问spark web界面  
```shell
google-chrome 172.18.0.10:8080
```

开启`spark-shell`   
出现`scala>`后，测试   
`println("Hello world")`  
结果 `Hello world`  
退出`spark-shell`  
`:q`或`:exit`  

### 使 用 spark on yarn 的 模 式 提 交 $SPARK_HOME/examples/jars/spark-examples_2.11-2.1.1.jar 运行的主类为 org.apache.spark.examples.SparkPi  
```shell
cd /usr/spark/spark-2.1.1-bin-hadoop2.7
bin/spark-submit --master yarn --class org.apache.spark.examples.SparkPi examples/jars/spark-examples_2.11-2.1.1.jar 100
```
# 任务三 安装flink
## 安装flink
```shell
mkdir -p /usr/flink
tar -zxvf /opt/package/flink-1.10.2-bin-scala_2.12.tgz -C /usr/flink
```
### 配置环境变量  
```shell
vim /etc/profile.d/flink.sh
```
`i` 进入编辑模式  
```shell 
#flink
export FLINK_HOME=/usr/flink/flink-1.10.2
export PATH=$FLINK_HOME/bin:$PATH
```
`esc` 退出编辑模式  
`:wq` 保存退出  
应用环境变量  
```shell
source /etc/profile
```
### 分发到slaves  
```shell
scp -r /usr/flink root@slave1:/usr
scp -r /usr/flink root@slave2:/usr
scp -r /etc/profile.d/flink.sh root@slave1:/etc/profile.d
scp -r /etc/profile.d/flink.sh root@slave2:/etc/profile.d
ssh root@slave1 'source /etc/profile'
ssh root@slave2 'source /etc/profile'
```

## Flink on Yark 使用per-job测试  
```shell
$FLINK_HOME/bin/start-cluster.sh
flink run -m yarn-cluster -p 2 -yjm 2G -ytm 2G $FLINK_HOME/examples/batch/WordCount.jar
```

| node |      |
|------|------|
|master|Client| 
|slave1| Hive |
|slave2| MySQL|